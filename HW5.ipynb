{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS454 HW5 CAN SÖLÖMBAZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((8,8,4))\n",
    "reward_table= np.zeros((8,8,4))\n",
    "#destination (6,5)\n",
    "reward_table[6,4,2]=100\n",
    "reward_table[6,6,3]=100\n",
    "reward_table[7,5,1]=100\n",
    "reward_table[5,5,0]=100\n",
    "\n",
    "epsilon=0.3\n",
    "gamma=0.9\n",
    "alpha=0.1\n",
    "def move(action,state):\n",
    "    x=state[0]\n",
    "    y=state[1]\n",
    "    if action==0: #up\n",
    "        y=y+1\n",
    "    elif action==1: #down\n",
    "        y=y-1\n",
    "    elif action==2: #right\n",
    "        x=x+1\n",
    "    elif action==3: #left\n",
    "        x=x-1\n",
    "    \n",
    "    nextstate=[state[0],state[1]]\n",
    "    if not (x<0 or x>7) and not (y<0 or y>7):\n",
    "        nextstate=[x,y]\n",
    "        out_of_board=False\n",
    "    else:\n",
    "        out_of_board=True\n",
    "        \n",
    "    return nextstate,out_of_board\n",
    "\n",
    "def q_learn():\n",
    "    for episode in range(1000):\n",
    "        state=[0,0]\n",
    "        while not state==[6,5]:\n",
    "            \n",
    "            out_of_board=True\n",
    "            while out_of_board==True:\n",
    "                if np.random.uniform(0,1)<epsilon:\n",
    "                    action=np.random.choice(4)\n",
    "                else:\n",
    "                    action=np.argmax(Q[state[0]][state[1]])\n",
    "\n",
    "                nextstate,out_of_board=move(action,state)\n",
    "           \n",
    "            reward=reward_table[state[0]][state[1]][action]\n",
    "            Q[state[0]][state[1]][action]=np.round((Q[state[0]][state[1]][action]+\n",
    "                alpha*(reward+gamma*max(Q[nextstate[0]][nextstate[1]])-Q[state[0]][state[1]][action])),3)\n",
    "            \n",
    "            state=nextstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
