{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW4 CS 454 CAN SÖLÖMBAZ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm as cm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=[i for i in range(10)]\n",
    "def conf_matrix(y_pred,y_true,label):\n",
    "    labels=label\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=label)\n",
    "    print(cm)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Confusion matrix')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticks(labels)\n",
    "    ax.set_yticks(labels)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This structure is very similar to LeNet-5. Only difference is size of the image which is 1x28x28 in our case\n",
    "## Structure of CNN as follows: 1 Convnet-1 Maxpool-1 Convnet-1 Maxpool-1 Convnet-Flatten-2 FulCon-Softmax\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # ====== ENCODER PART ======       \n",
    "        # MNIST image is 1x28x28 (CxHxW)\n",
    "        # Pytorch expects input data as BxCxHxW \n",
    "        # B: Batch size\n",
    "        # C: number of channels gray scale images have 1 channel\n",
    "        # W: width of the image \n",
    "        # H: height of the image\n",
    "        \n",
    "        # W after conv2d  [(W - Kernelw + 2*padding)/stride] + 1\n",
    "        \n",
    "        # after convolution we'll have Bx6 24x24 feature maps\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,\n",
    "                                out_channels=6,\n",
    "                                kernel_size=5,\n",
    "                                stride=1,\n",
    "                                padding=0)\n",
    "        \n",
    "        # after maxpool we'll have Bx6 12x12 feature maps\n",
    "        self.maxpool1 = nn.MaxPool2d(2,2)\n",
    "\n",
    "\n",
    "        \n",
    "        # after convolution we'll have Bx16 8x8 feature maps \n",
    "        self.conv2= nn.Conv2d(in_channels=6,\n",
    "                                out_channels=16,\n",
    "                                kernel_size=5,\n",
    "                                stride=1,\n",
    "                                padding=0\n",
    "                                )\n",
    "\n",
    "        # after maxpool we'll have Bx16 4x4 feature maps\n",
    "        self.maxpool2 = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.conv3= nn.Conv2d(in_channels=16,\n",
    "                                out_channels=120,\n",
    "                                kernel_size=4,\n",
    "                                stride=1,\n",
    "                                padding=0\n",
    "                                )\n",
    "        \n",
    "        # first fully connected layer from 120 input features to 84 hidden units\n",
    "        self.fc1 = nn.Linear(in_features=120,\n",
    "                                out_features=84)\n",
    "       \n",
    "        # second fully connected layer from 84 input features to 10 outputs\n",
    "        self.fc2 = nn.Linear(in_features=84,\n",
    "                                out_features=10)\n",
    "                           \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.flatten(x,1) # flatten feature maps, Bx (CxHxW)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x),dim=1)\n",
    "#         x = x.view(-1,64,4,4) # reshape back to feature map format\n",
    "\n",
    "        return x        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Can_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Can_CNN, self).__init__()\n",
    "        \n",
    "        # W after conv2d  [(W - Kernelw + 2*padding)/stride] + 1\n",
    "        \n",
    "        # after convolution we'll have Bx32 28x28 feature maps\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,\n",
    "                                out_channels=32,\n",
    "                                kernel_size=5,\n",
    "                                stride=1,\n",
    "                                padding=2)\n",
    "\n",
    "        # after avgpool we'll have Bx32 14x14 feature maps\n",
    "        self.maxpool1 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        \n",
    "        # after convolution we'll have Bx64 14x14 feature maps \n",
    "        self.conv2= nn.Conv2d(in_channels=32,\n",
    "                                out_channels=64,\n",
    "                                kernel_size=5,\n",
    "                                stride=1,\n",
    "                                padding=2\n",
    "                                )    \n",
    "        \n",
    "        # after avgpool we'll have Bx64 7x7 feature maps\n",
    "        self.maxpool2 = nn.MaxPool2d(2,2)\n",
    "               \n",
    "        \n",
    "        # first fully connected layer from 64*7*7 input features to 128 hidden units\n",
    "        self.fc1 = nn.Linear(in_features=64*7*7,\n",
    "                                out_features=128)\n",
    "       \n",
    "        # second fully connected layer from 128 input features to 10 outputs\n",
    "        self.fc2 = nn.Linear(in_features=128,\n",
    "                                out_features=10)\n",
    "        \n",
    "                           \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool2(x)\n",
    "        x = torch.flatten(x,1) # flatten feature maps, Bx (CxHxW)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x),dim=1)\n",
    "#         x = x.view(-1,64,4,4) # reshape back to feature map format\n",
    "\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)   # from [-1, 1] range to [0, 1] range\n",
    "    x = x.clamp(0, 1)   # assign less than 0 to 0, bigger than 1 to 1\n",
    "    x = x.view(x.size(0), 1, 28, 28) # B, C, H, W format for MNIST\n",
    "    return x\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "n_batches = 60000 // batch_size\n",
    "\n",
    "# normalize each image and set the pixel values between -1 and 1\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# prepare data loader\n",
    "trainset = MNIST('./data', transform=img_transform, download=True)\n",
    "dataloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# determine where to run the code\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# create an CNN network instance\n",
    "net = CNN().to(device)\n",
    "net2 = Can_CNN().to(device)\n",
    "# print(net)  # display the architecture\n",
    "loss_function = nn.NLLLoss().to(device)\n",
    "optimizer1 = torch.optim.Adam(net.parameters(), lr=learning_rate,\n",
    "                             weight_decay=1e-5)\n",
    "optimizer2 = torch.optim.Adam(net2.parameters(), lr=learning_rate,\n",
    "                             weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, loader, loss_func, optimizer):\n",
    "    net.train()                           # put model in train mode\n",
    "    total_loss = torch.zeros(1).to(device)\n",
    "    \n",
    "    for img, target in loader:                  # next batch\n",
    "        img = Variable(img).to(device)     # convert to Variable to calculate gradient and move to gpu\n",
    "        target = Variable(target).to(device)\n",
    "        output = net(img).to(device)          # feed forward\n",
    "        loss = loss_func(output, target)      # calculate loss         \n",
    "        optimizer.zero_grad()               # clear previous gradients \n",
    "        loss.backward()                     # calculate new gradients\n",
    "        optimizer.step()                    # update weights \n",
    "        total_loss += loss                  # accumulate loss  \n",
    "            \n",
    "    return img, output, total_loss"
   ]
  },
  
   
   "source": [
    "for epoch in range(num_epochs):\n",
    "    img, output, loss = train(net, dataloader, loss_function, optimizer1)\n",
    "               \n",
    "    # log\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "            .format(epoch+1, num_epochs, loss.item()/n_batches))\n",
    "\n",
    "num_of_epoch=[i+1 for i in range(num_epochs)]\n",
    "plt.xticks(num_of_epoch)\n",
    "plt.xlabel(\"number of epoch\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.title(\"Epoch vs Loss\")\n",
    "plt.plot(num_of_epoch, errors) \n",
    "\n",
    "# save the model\n",
    "torch.save(net.state_dict(), './CNN.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = MNIST('./data', transform=img_transform, download=True, train=False)\n",
    "trainset = MNIST('./data', transform=img_transform, download=True)\n",
    "test_loader = DataLoader(testset, batch_size=10000, shuffle=True)\n",
    "train_loader = DataLoader(trainset, batch_size=60000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum')  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            confusion_matrix=conf_matrix(pred,target,label)\n",
    "            \n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 8, 4,  ..., 7, 5, 6])\n",
      "60000\n",
      "tensor([[1],\n",
      "        [8],\n",
      "        [4],\n",
      "        ...,\n",
      "        [7],\n",
      "        [5],\n",
      "        [6]])\n",
      "60000\n"
     ]
    },
    
   "source": [
    "results(net, device, train_loader)\n",
    "results(net, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
